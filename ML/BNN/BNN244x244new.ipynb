{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8347e929",
   "metadata": {},
   "source": [
    "# Memory-Efficient Binary Neural Network (BNN) - 244×244 Images\n",
    "\n",
    "This notebook extends the optimized 1-hidden layer BNN to 244×244 images. It includes progress bars via tqdm and comprehensive visualizations: ROC curves, Precision-Recall curves, training vs. validation accuracy, and epoch timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296577b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "# %pip install torch torchvision matplotlib scikit-learn tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187a1ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured for 244x244 images, batch size 8\n"
     ]
    }
   ],
   "source": [
    "# Memory Optimization Settings\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "\n",
    "# Use moderate/aggressive as needed\n",
    "def optimize_memory(mode='aggressive'):\n",
    "    if mode == 'aggressive': return {'image_size':244, 'batch_size':8, 'hidden_size':256, 'embedding_size':512, 'num_hidden_layers':1, 'gradient_accumulation':4}\n",
    "    else: return {'image_size':244, 'batch_size':16, 'hidden_size':320, 'embedding_size':640, 'num_hidden_layers':1, 'gradient_accumulation':2}\n",
    "\n",
    "memory_config = optimize_memory('aggressive')\n",
    "image_size = memory_config['image_size']\n",
    "batch_size = memory_config['batch_size']\n",
    "print(f\"Configured for {image_size}x{image_size} images, batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a43639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Healthy_Soyabean', 'Soyabean Semilooper_Pest_Attack', 'Soyabean_Mosaic', 'rust']\n",
      "Train: 1989 samples, Val: 426, Test: 427\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "# dataset_path = \"/path/to/plant-disease-dataset\"  # update path\n",
    "dataset_path = \"/home/dragoon/Downloads/MH-SoyaHealthVision An Indian UAV and Leaf Image Dataset for Integrated Crop Health Assessment/Soyabean_UAV-Based_Image_Dataset\"  # update path\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Split train/val/test (70/15/15)\n",
    "total = len(full_dataset)\n",
    "train_len = int(0.7*total)\n",
    "val_len = int(0.15*total)\n",
    "test_len = total - train_len - val_len\n",
    "train_ds, val_ds, test_ds = torch.utils.data.random_split(full_dataset,[train_len,val_len,test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd8422c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 31900817664 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m input_size = \u001b[32m3\u001b[39m*image_size*image_size\n\u001b[32m     24\u001b[39m num_classes = \u001b[38;5;28mlen\u001b[39m(class_names)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m model = \u001b[43mOptimizedBNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43membedding_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mOptimizedBNN.__init__\u001b[39m\u001b[34m(self, input_size, hidden_size, num_classes, embedding_size, num_hidden_layers, dropout_rate)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# embedding\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding = nn.Sequential(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m, nn.ReLU(), nn.BatchNorm1d(input_size//\u001b[32m4\u001b[39m), nn.Dropout(dropout_rate),\n\u001b[32m      8\u001b[39m     nn.Linear(input_size//\u001b[32m4\u001b[39m, embedding_size), nn.ReLU(), nn.BatchNorm1d(embedding_size)\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.input_binary = nn.Linear(embedding_size, hidden_size)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hidden_layers)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/drone-crop/ML/BNN/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 31900817664 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Define Optimized BNN Model\n",
    "class OptimizedBNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, embedding_size, num_hidden_layers=1, dropout_rate=0.35):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size//4), nn.ReLU(), nn.BatchNorm1d(input_size//4), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(input_size//4, embedding_size), nn.ReLU(), nn.BatchNorm1d(embedding_size)\n",
    "        )\n",
    "        self.input_binary = nn.Linear(embedding_size, hidden_size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)])\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.embedding(x)\n",
    "        x = torch.sign(self.input_binary(x))\n",
    "        for layer in self.hidden:\n",
    "            x = torch.sign(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        return self.output(x)\n",
    "\n",
    "input_size = 3*image_size*image_size\n",
    "num_classes = len(class_names)\n",
    "model = OptimizedBNN(input_size, memory_config['hidden_size'], num_classes, memory_config['embedding_size']).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function with tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_validate(model, train_loader, val_loader, epochs):\n",
    "    history = {'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[],'epoch_time':[]}\n",
    "    # Early stopping parameters\n",
    "    best_val_acc = 0.0\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        # TRAIN\n",
    "        model.train(); running_loss=0; correct=0; total=0\n",
    "        for x,y in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\"):  \n",
    "            x,y = x.to(device),y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(): outputs = model(x); loss = criterion(outputs,y)\n",
    "            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "            running_loss += loss.item()*y.size(0)\n",
    "            pred = outputs.argmax(1); correct += (pred==y).sum().item(); total+=y.size(0)\n",
    "        train_loss = running_loss/total; train_acc=100*correct/total\n",
    "        # VAL\n",
    "        model.eval(); v_loss=0; v_corr=0; v_tot=0\n",
    "        all_probs=[]; all_targets=[]\n",
    "        with torch.no_grad():\n",
    "            for x,y in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\"):  \n",
    "                x,y = x.to(device),y.to(device)\n",
    "                out = model(x); loss = criterion(out,y)\n",
    "                v_loss += loss.item()*y.size(0)\n",
    "                p = out.argmax(1); v_corr += (p==y).sum().item(); v_tot+=y.size(0)\n",
    "                all_probs.append(F.softmax(out,1).cpu().numpy()); all_targets.append(y.cpu().numpy())\n",
    "        val_loss = v_loss/v_tot; val_acc=100*v_corr/v_tot\n",
    "        scheduler.step()\n",
    "        # Early stopping: track best validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if 'early_stopping_patience' in locals() and no_improve >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}, best val_acc={best_val_acc:.2f}%\")\n",
    "            break\n",
    "\n",
    "        et = time.time()-start\n",
    "        history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "        history['epoch_time'].append(et)\n",
    "        print(f\"Epoch {epoch}: train_acc={train_acc:.2f}%, val_acc={val_acc:.2f}%, time={et:.2f}s\")\n",
    "    # Load best model weights if available\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"Loaded best model weights with val_acc={best_val_acc:.2f}%\")\n",
    "\n",
    "    return history, np.vstack(all_probs), np.concatenate(all_targets)\n",
    "\n",
    "history, probs, targets = train_validate(model, train_loader, val_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss'); plt.legend(); plt.title('Loss')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc'); plt.legend(); plt.title('Accuracy')\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(range(1,len(history['epoch_time'])+1), history['epoch_time']); plt.title('Time/epoch'); plt.xlabel('Epoch'); plt.ylabel('Seconds')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90747574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall Curves\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize targets\n",
    "y_bin = label_binarize(targets, classes=list(range(num_classes)))\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "precision, recall, pr_auc = dict(), dict(), dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin[:,i], probs[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_bin[:,i], probs[:,i])\n",
    "    pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{class_names[i]} (AUC={roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--'); plt.title('ROC Curves'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.show()\n",
    "\n",
    "# Plot PR\n",
    "plt.figure(figsize=(6,5))\n",
    "for i in range(num_classes):\n",
    "    plt.plot(recall[i], precision[i], label=f\"{class_names[i]} (AUC={pr_auc[i]:.2f})\")\n",
    "plt.title('Precision-Recall Curves'); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11ff15",
   "metadata": {},
   "source": [
    "**End of Notebook: BNN244x244new.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import os\n",
    "save_dir = 'results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'bnn_244x244_model_new.pt')\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
