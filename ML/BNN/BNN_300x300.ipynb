{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdcc20d",
   "metadata": {},
   "source": [
    "# Memory-Efficient Binary Neural Network (BNN) for Plant Disease Classification - 300x300 Images\n",
    "\n",
    "This notebook implements a Binary Neural Network using PyTorch for multiclass plant disease classification, optimized for handling 300x300 images on GPUs with limited memory.\n",
    "\n",
    "## Memory Management Guide\n",
    "\n",
    "This notebook has been optimized for running on GPUs with limited memory (under 4GB). The settings have been automatically adjusted to \"aggressive\" mode to prevent CUDA out-of-memory errors.\n",
    "\n",
    "### Current Memory-Optimized Settings:\n",
    "- Image resolution: 300x300 (higher resolution)\n",
    "- Batch size: 2 (reduced for larger images)\n",
    "- Hidden size: 256 (compact model architecture)\n",
    "- Progressive dimensionality reduction (multiple embedding steps)\n",
    "- Gradient accumulation: 12 steps (effective batch size of 24)\n",
    "- Mixed precision training (FP16)\n",
    "- Sample limiting: 50 per class (for faster training with high-res images)\n",
    "\n",
    "### Memory Monitoring\n",
    "You can monitor GPU memory usage by checking the output of the memory monitoring tools included at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82462e",
   "metadata": {},
   "source": [
    "# Binary Neural Network (BNN) for Plant Disease Classification\n",
    "\n",
    "This notebook implements a Binary Neural Network using PyTorch for multiclass plant disease classification. The BNN uses binary weights and activations to reduce model size and computational requirements while maintaining reasonable accuracy.\n",
    "\n",
    "## Features:\n",
    "- Binary weights and activations using sign function\n",
    "- Processes 300x300 RGB images (higher resolution)\n",
    "- Progressive dimensionality reduction for memory efficiency\n",
    "- Binary hidden layers with binary weights\n",
    "- Batch normalization for improved stability\n",
    "- Dropout for regularization\n",
    "- Learning rate scheduling for better convergence\n",
    "- Multiclass output with softmax activation\n",
    "- Mixed precision training\n",
    "- Gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Memory configuration - optimized for large images (300x300)\n",
    "memory_config = {\n",
    "    'batch_size': 12,  # Smaller batch size for larger images\n",
    "    'gradient_accumulation_steps': 4,  # Accumulate gradients over multiple batches\n",
    "    'gc_frequency': 5,  # Garbage collection frequency\n",
    "    'memory_efficient': True,  # Use memory-efficient techniques\n",
    "    'use_mixed_precision': True  # Use mixed precision training if available\n",
    "}\n",
    "\n",
    "# Use memory configuration\n",
    "batch_size = memory_config['batch_size']\n",
    "gradient_accumulation_steps = memory_config['gradient_accumulation_steps'] \n",
    "gc_frequency = memory_config['gc_frequency']\n",
    "memory_efficient = memory_config['memory_efficient']\n",
    "use_mixed_precision = memory_config['use_mixed_precision']\n",
    "memory_mode = 'High Memory Optimization (300x300)'\n",
    "\n",
    "print(f\"Memory Configuration Mode: {memory_mode}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective Batch Size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"GC Frequency: {gc_frequency}\")\n",
    "print(f\"Memory Efficient: {memory_efficient}\")\n",
    "print(f\"Mixed Precision: {use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for enhanced visualization and data export\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(\"Results directory created: ./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98136b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bnn_memory_efficient(model, test_loader, criterion, device, class_names, batch_limit=None, use_mixed_precision=False):\n",
    "    \"\"\"\n",
    "    Memory-efficient evaluation function for BNN model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained BNN model\n",
    "        test_loader: DataLoader for the test dataset\n",
    "        criterion: Loss function\n",
    "        device: Device to run evaluation on\n",
    "        class_names: List of class names\n",
    "        batch_limit: Limit the number of batches to evaluate (for debugging)\n",
    "        use_mixed_precision: Whether to use mixed precision\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Set up metrics\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize confusion matrix\n",
    "    num_classes = len(class_names)\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    \n",
    "    # Prepare for per-class metrics\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    # Set up for precision, recall, F1\n",
    "    true_positives = [0] * num_classes\n",
    "    false_positives = [0] * num_classes\n",
    "    false_negatives = [0] * num_classes\n",
    "    \n",
    "    # Store some sample images for visualization\n",
    "    sample_images = []\n",
    "    sample_labels = []\n",
    "    sample_preds = []\n",
    "    samples_collected = 0\n",
    "    max_samples = 10  # Maximum number of samples to collect\n",
    "    \n",
    "    # For ROC curve\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Check if mixed precision is available\n",
    "    mixed_precision_available = use_mixed_precision and hasattr(torch, 'autocast')\n",
    "    \n",
    "    # Determine the appropriate autocast context manager based on PyTorch version\n",
    "    if mixed_precision_available:\n",
    "        try:\n",
    "            from torch.cuda.amp import autocast\n",
    "            # Check if the version supports device_type parameter\n",
    "            import torch\n",
    "            torch_version = torch.__version__\n",
    "            supports_device_type = int(torch_version.split('.')[0]) >= 1 and int(torch_version.split('.')[1]) >= 10\n",
    "            \n",
    "            # Define context manager with appropriate parameters\n",
    "            if supports_device_type:\n",
    "                autocast_context = lambda: autocast(device_type=device.type)\n",
    "            else:\n",
    "                # Older PyTorch versions only support CUDA and don't need device_type\n",
    "                autocast_context = lambda: autocast()\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from torch.amp import autocast\n",
    "                autocast_context = lambda: autocast(device_type=device.type)\n",
    "            except ImportError:\n",
    "                mixed_precision_available = False\n",
    "                import contextlib\n",
    "                autocast_context = contextlib.nullcontext\n",
    "    else:\n",
    "        import contextlib\n",
    "        autocast_context = contextlib.nullcontext\n",
    "\n",
    "    # Track processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Process batches\n",
    "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "            # Respect batch limit if specified\n",
    "            if batch_limit is not None and batch_idx >= batch_limit:\n",
    "                print(f\"Evaluating model on {batch_limit} batches (limited)...\")\n",
    "                break\n",
    "                \n",
    "            if batch_idx == 0:\n",
    "                print(f\"Evaluating model on {len(test_loader)} batches (all)...\")\n",
    "                \n",
    "            # Move data to device\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Use mixed precision if available\n",
    "            if mixed_precision_available:\n",
    "                with autocast_context():\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            # Accumulate loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # Update metrics\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Store probabilities for ROC curve (using softmax)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            for t, p in zip(targets.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "            # Update per-class metrics\n",
    "            for i in range(len(targets)):\n",
    "                label = targets[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                class_total[label] += 1\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                    \n",
    "            # Update precision, recall metrics\n",
    "            for c in range(num_classes):\n",
    "                true_positives[c] += ((predicted == c) & (targets == c)).sum().item()\n",
    "                false_positives[c] += ((predicted == c) & (targets != c)).sum().item()\n",
    "                false_negatives[c] += ((predicted != c) & (targets == c)).sum().item()\n",
    "                \n",
    "            # Collect sample images for visualization\n",
    "            if samples_collected < max_samples:\n",
    "                # Get a few samples from this batch\n",
    "                num_to_collect = min(max_samples - samples_collected, data.size(0))\n",
    "                sample_images.extend(data[:num_to_collect].cpu())\n",
    "                sample_labels.extend(targets[:num_to_collect].cpu().numpy())\n",
    "                sample_preds.extend(predicted[:num_to_collect].cpu().numpy())\n",
    "                samples_collected += num_to_collect\n",
    "                \n",
    "            # Clean up memory\n",
    "            del data, targets, outputs, predicted\n",
    "            \n",
    "    # Compute average loss\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Compute per-class accuracy\n",
    "    class_accuracy = [100. * class_correct[i] / max(1, class_total[i]) for i in range(num_classes)]\n",
    "    \n",
    "    # Compute precision, recall, F1\n",
    "    precision = [true_positives[i] / max(1, true_positives[i] + false_positives[i]) for i in range(num_classes)]\n",
    "    recall = [true_positives[i] / max(1, true_positives[i] + false_negatives[i]) for i in range(num_classes)]\n",
    "    f1_score = [2 * precision[i] * recall[i] / max(1e-6, precision[i] + recall[i]) for i in range(num_classes)]\n",
    "    \n",
    "    # Calculate macro averages\n",
    "    macro_precision = sum(precision) / num_classes\n",
    "    macro_recall = sum(recall) / num_classes\n",
    "    macro_f1 = sum(f1_score) / num_classes\n",
    "    \n",
    "    # Convert confusion matrix to percentage\n",
    "    confusion_percentage = confusion_matrix.diag() / confusion_matrix.sum(1)\n",
    "    \n",
    "    # Track total processing time\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # Return a comprehensive metrics dictionary\n",
    "    metrics = {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'class_accuracy': dict(zip(class_names, class_accuracy)),\n",
    "        'precision': dict(zip(class_names, precision)),\n",
    "        'recall': dict(zip(class_names, recall)),\n",
    "        'f1_score': dict(zip(class_names, f1_score)),\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'confusion_matrix': confusion_matrix.cpu().numpy(),\n",
    "        'confusion_percentage': confusion_percentage.cpu().numpy(),\n",
    "        'class_distribution': dict(zip(class_names, class_total)),\n",
    "        'evaluation_time': eval_time,\n",
    "        'samples': {\n",
    "            'images': sample_images,\n",
    "            'true_labels': sample_labels,\n",
    "            'predicted_labels': sample_preds,\n",
    "        },\n",
    "        'roc_data': {\n",
    "            'targets': all_targets,\n",
    "            'probs': all_probs,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35404a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-Efficient Training Function with Version-Compatible Autocast\n",
    "def train_memory_efficient(model, train_loader, criterion, optimizer, num_epochs, device,\n",
    "                          scheduler=None, gradient_accumulation_steps=1, memory_efficient=True,\n",
    "                          gc_frequency=10, use_mixed_precision=False, \n",
    "                          early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient training function for BNN model\n",
    "    \n",
    "    Args:\n",
    "        model: The BNN model\n",
    "        train_loader: DataLoader for the training dataset\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for training\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to train on (cpu or cuda)\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "        memory_efficient: Whether to use memory efficiency techniques\n",
    "        gc_frequency: How often to perform garbage collection\n",
    "        use_mixed_precision: Whether to use mixed precision training\n",
    "        early_stopping_patience: Patience for early stopping (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    # Training history\n",
    "    history = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    # For early stopping\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # For timing\n",
    "    epoch_times = []\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    mixed_precision_available = use_mixed_precision and hasattr(torch, 'autocast')\n",
    "    \n",
    "    # Setup mixed precision tools if available\n",
    "    if mixed_precision_available:\n",
    "        try:\n",
    "            from torch.cuda.amp import autocast, GradScaler\n",
    "            scaler = GradScaler()\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from torch.amp import autocast, GradScaler\n",
    "                scaler = GradScaler()\n",
    "            except ImportError:\n",
    "                mixed_precision_available = False\n",
    "                scaler = None\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    # Determine the appropriate autocast context manager based on PyTorch version\n",
    "    if mixed_precision_available:\n",
    "        try:\n",
    "            from torch.cuda.amp import autocast\n",
    "            # Check if the version supports device_type parameter\n",
    "            import torch\n",
    "            torch_version = torch.__version__\n",
    "            supports_device_type = int(torch_version.split('.')[0]) >= 1 and int(torch_version.split('.')[1]) >= 10\n",
    "            \n",
    "            # Define context manager with appropriate parameters\n",
    "            if supports_device_type:\n",
    "                autocast_context = lambda: autocast(device_type=device.type)\n",
    "            else:\n",
    "                # Older PyTorch versions only support CUDA and don't need device_type\n",
    "                autocast_context = lambda: autocast()\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from torch.amp import autocast\n",
    "                autocast_context = lambda: autocast(device_type=device.type)\n",
    "            except ImportError:\n",
    "                mixed_precision_available = False\n",
    "                import contextlib\n",
    "                autocast_context = contextlib.nullcontext\n",
    "    else:\n",
    "        import contextlib\n",
    "        autocast_context = contextlib.nullcontext\n",
    "    \n",
    "    # Main training loop\n",
    "    print(f\"Starting training for {num_epochs} epochs with mixed precision: {mixed_precision_available}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Reset gradients at the start of each epoch for consistent behavior\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process batches\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision if available\n",
    "            if mixed_precision_available:\n",
    "                with autocast_context():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Adjust loss for gradient accumulation\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Step with gradient accumulation\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                # Standard forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Adjust loss for gradient accumulation\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "                # Standard backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Step with gradient accumulation\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * gradient_accumulation_steps  # Rescale loss for reporting\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if memory_efficient and (batch_idx + 1) % gc_frequency == 0:\n",
    "                del inputs, labels, outputs, preds, loss\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "        # Make sure to step optimizer for the last batch if not divisible\n",
    "        if mixed_precision_available and train_loader.__len__() % gradient_accumulation_steps != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        elif not mixed_precision_available and train_loader.__len__() % gradient_accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects / total_samples * 100.0\n",
    "        \n",
    "        # Step scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "        # Record metrics\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Record epoch time\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%, Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Save epoch history\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "            'time': epoch_time,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        # Memory cleanup at the end of epoch\n",
    "        if memory_efficient:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        # Early stopping check\n",
    "        if early_stopping_patience is not None:\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "                \n",
    "    # Return training metrics\n",
    "    training_summary = {\n",
    "        'history': history,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'epoch_times': epoch_times,\n",
    "        'total_time': sum(epoch_times),\n",
    "        'final_loss': train_losses[-1],\n",
    "        'final_accuracy': train_accuracies[-1]\n",
    "    }\n",
    "    \n",
    "    return training_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77939d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Monitoring and Optimization Utilities\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "def print_gpu_memory_stats():\n",
    "    \"\"\"Print detailed GPU memory statistics\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        \n",
    "        print(f\"Memory allocated: {allocated:.2f} GB\")\n",
    "        print(f\"Memory reserved: {reserved:.2f} GB\")\n",
    "        print(f\"Max memory allocated: {max_allocated:.2f} GB\")\n",
    "        \n",
    "        if hasattr(torch.cuda, 'memory_summary'):\n",
    "            print(\"\\nMemory Summary:\")\n",
    "            print(torch.cuda.memory_summary(abbreviated=True))\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "def print_system_memory():\n",
    "    \"\"\"Print system memory usage\"\"\"\n",
    "    vm = psutil.virtual_memory()\n",
    "    print(f\"System memory: {vm.total / (1024**3):.1f} GB total, \" \n",
    "          f\"{vm.available / (1024**3):.1f} GB available, \"\n",
    "          f\"{vm.percent}% used\")\n",
    "\n",
    "def optimize_memory(mode='aggressive'):\n",
    "    \"\"\"Apply memory optimization settings based on selected mode\"\"\"\n",
    "    if mode == 'aggressive':\n",
    "        # Most aggressive memory saving settings for 300x300 images\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        return {\n",
    "            'image_size': 300,  # 300x300 for this notebook\n",
    "            'batch_size': 2,  # Very small batch size for larger images\n",
    "            'hidden_size': 256,\n",
    "            'embedding_size': 512,\n",
    "            'num_hidden_layers': 1,\n",
    "            'gradient_accumulation': 12,\n",
    "            'max_samples_per_class': 50  # Limit samples for 300x300 images\n",
    "        }\n",
    "    elif mode == 'moderate':\n",
    "        # Balanced memory saving\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:64'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        return {\n",
    "            'image_size': 300,  # 300x300 for this notebook\n",
    "            'batch_size': 3,\n",
    "            'hidden_size': 320,\n",
    "            'embedding_size': 640,\n",
    "            'num_hidden_layers': 1,\n",
    "            'gradient_accumulation': 8,\n",
    "            'max_samples_per_class': 100\n",
    "        }\n",
    "    else:  # 'performance' mode\n",
    "        # Optimized for performance, higher memory usage\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        return {\n",
    "            'image_size': 300,  # 300x300 for this notebook\n",
    "            'batch_size': 4,\n",
    "            'hidden_size': 512,\n",
    "            'embedding_size': 1024,\n",
    "            'num_hidden_layers': 2,\n",
    "            'gradient_accumulation': 4,\n",
    "            'max_samples_per_class': None\n",
    "        }\n",
    "\n",
    "# Check current memory status\n",
    "print(\"Initial memory status:\")\n",
    "print_system_memory()\n",
    "print_gpu_memory_stats()\n",
    "\n",
    "# Memory optimization mode - set this to 'performance', 'moderate', or 'aggressive'\n",
    "memory_mode = 'aggressive'  # Using aggressive mode to avoid CUDA OOM errors\n",
    "print(f\"\\nUsing {memory_mode} memory optimization settings\")\n",
    "memory_config = optimize_memory(memory_mode)\n",
    "print(f\"Recommended settings: {memory_config}\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(\"\\nResults directory created: ./results/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
