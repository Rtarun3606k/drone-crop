{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Binary Neural Network with Dual Attention for Disease Classification\n",
    "Based on: \"A Binary Neural Network with Dual Attention for Plant Disease Classification\"\n",
    "DOI: https://doi.org/10.3390/electronics12214431\n",
    "\n",
    "This implementation trains a BNN model to classify diseases into 4 classes using 64000 images (16000 per class).\n",
    "Optimized for GPU training with progress tracking and maximum accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for maximum GPU utilization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters optimized for 4GB GPU memory\n",
    "BATCH_SIZE = 32  # Reduced from 128 to fit in GPU memory\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "NUM_CLASSES = 4\n",
    "IMG_SIZE = 128  # Reduced from 224 to save memory\n",
    "NUM_WORKERS = 4  # Reduced to save system memory\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# Data splits (70% train, 20% val, 10% test for maximum training data)\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78edfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryActivation(torch.autograd.Function):\n",
    "    \"\"\"Binary activation function for BNN\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.sign(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.le(-1)] = 0\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLinear(nn.Module):\n",
    "    \"\"\"Binary linear layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        binary_weight = torch.sign(self.weight)\n",
    "        output = F.linear(input, binary_weight, self.bias)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryConv2d(nn.Module):\n",
    "    \"\"\"Binary convolution layer\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super(BinaryConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        binary_weight = torch.sign(self.weight)\n",
    "        output = F.conv2d(input, binary_weight, self.bias, self.stride, self.padding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention mechanism from the paper\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        \n",
    "        # Channel attention\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        \n",
    "        return self.sigmoid(out).view(b, c, 1, 1) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism from the paper\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        \n",
    "        out = self.conv(x_cat)\n",
    "        return self.sigmoid(out) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualAttention(nn.Module):\n",
    "    \"\"\"Dual attention combining channel and spatial attention\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(DualAttention, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DABConv(nn.Module):\n",
    "    \"\"\"Dual Attention Binary Convolution module\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(DABConv, self).__init__()\n",
    "        self.binary_conv = BinaryConv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dual_attention = DualAttention(out_channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.binary_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dual_attention(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNNBasicBlock(nn.Module):\n",
    "    \"\"\"BNN Basic Block with residual connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BNNBasicBlock, self).__init__()\n",
    "        \n",
    "        # First path\n",
    "        self.dabconv1 = DABConv(in_channels, out_channels, 3, stride, 1)\n",
    "        self.dabconv2 = DABConv(out_channels, out_channels, 1, 1, 0)\n",
    "        \n",
    "        # Second path\n",
    "        self.dabconv3 = DABConv(out_channels, out_channels, 3, 1, 1)\n",
    "        self.dabconv4 = DABConv(out_channels, out_channels, 1, 1, 0)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = self.dabconv1(x)\n",
    "        out = self.dabconv2(out)\n",
    "        out = self.dabconv3(out)\n",
    "        out = self.dabconv4(out)\n",
    "        \n",
    "        out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DABNN(nn.Module):\n",
    "    \"\"\"Dual Attention Binary Neural Network\"\"\"\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(DABNN, self).__init__()\n",
    "        \n",
    "        # Stem block\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False),  # Initial quantized conv\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            DABConv(64, 64, 3, 1, 1),  # 3x3 DABconv\n",
    "            DABConv(64, 128, 1, 2, 0)  # 1x1 DABconv with downsampling\n",
    "        )\n",
    "        \n",
    "        # Feature extractor - 6 BNN basic blocks\n",
    "        self.features = nn.ModuleList([\n",
    "            BNNBasicBlock(128, 128),\n",
    "            BNNBasicBlock(128, 256, 2),\n",
    "            BNNBasicBlock(256, 256),\n",
    "            BNNBasicBlock(256, 512, 2),\n",
    "            BNNBasicBlock(512, 512),\n",
    "            BNNBasicBlock(512, 512)\n",
    "        ])\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.PReLU(),\n",
    "            BinaryLinear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for block in self.features:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f474364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transforms():\n",
    "    \"\"\"Optimized data augmentation for maximum accuracy\"\"\"\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c74200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_path):\n",
    "    \"\"\"Create optimized data loaders\"\"\"\n",
    "    train_transforms, val_test_transforms = get_data_transforms()\n",
    "    \n",
    "    # Load dataset\n",
    "    full_dataset = ImageFolder(data_path, transform=train_transforms)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(TRAIN_RATIO * total_size)\n",
    "    val_size = int(VAL_RATIO * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Apply appropriate transforms\n",
    "    val_dataset.dataset.transform = val_test_transforms\n",
    "    test_dataset.dataset.transform = val_test_transforms\n",
    "    \n",
    "    # Create data loaders with optimized settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, full_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1170a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device, class_names):\n",
    "    \"\"\"Test the final model\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_acc = 100.0 * correct / total\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    return test_acc, report, cm, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47951f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o', markersize=3)\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s', markersize=3)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accs, label='Train Accuracy', marker='o', markersize=3)\n",
    "    ax2.plot(val_accs, label='Val Accuracy', marker='s', markersize=3)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count model parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: {total_params * 4 / (1024**2):.2f} MB (FP32)\")\n",
    "    print(f\"Binary model size (estimated): {total_params / (8 * 1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0457a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Set up data path (modify this to your dataset path)\n",
    "    data_path = \"/home/bruh/Documents/BNN2/split\"  # Change this to your actual dataset path\n",
    "    \n",
    "    # Verify CUDA optimization\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes\n",
    "        torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BNN Disease Classification Training\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, class_names = create_data_loaders(data_path)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\\\nInitializing DABNN model...\")\n",
    "    model = DABNN(num_classes=NUM_CLASSES).to(device)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for better generalization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"\\\\nStarting training...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Record history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1:3d}/{NUM_EPOCHS}] | \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"LR: {current_lr:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}%\"\n",
    "              f\"{' *' if val_acc == best_val_acc else ''}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epoch > 20 and val_acc < max(val_accs[-10:]) - 5:  # Stop if no improvement in 10 epochs\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\\\nTesting best model...\")\n",
    "    test_acc, report, cm, preds, targets = test_model(model, test_loader, device, class_names)\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "    print(\"\\\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state,\n",
    "        'model_config': {\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'img_size': IMG_SIZE\n",
    "        },\n",
    "        'class_names': class_names,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc\n",
    "    }, 'best_dabnn_model.pth')\n",
    "    print(\"\\\\nBest model saved as 'best_dabnn_model.pth'\")\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\\\nGenerating plots...\")\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "    \n",
    "    # Final model statistics\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Total Training Time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Model Size (Binary): {sum(p.numel() for p in model.parameters()) / (8 * 1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8674a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detailed_analysis(model, test_loader, device, class_names):\n",
    "    \"\"\"Generate detailed performance analysis plots including ROC curves\"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from itertools import cycle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    # Binarize the targets for multiclass ROC\n",
    "    all_targets_bin = label_binarize(all_targets, classes=range(len(class_names)))\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. ROC Curves (Multi-class)\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_targets_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_targets_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.3f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curves')\n",
    "    plt.legend(loc=\"lower right\", fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision-Recall Curves\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(all_targets_bin[:, i], all_probs[:, i])\n",
    "        pr_auc[i] = average_precision_score(all_targets_bin[:, i], all_probs[:, i])\n",
    "    \n",
    "    # Plot PR curves\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                 label=f'{class_names[i]} (AP = {pr_auc[i]:.3f})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\", fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Class-wise Accuracy Bar Plot\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    class_accuracies = []\n",
    "    for i in range(n_classes):\n",
    "        class_mask = all_targets == i\n",
    "        class_acc = (all_preds[class_mask] == all_targets[class_mask]).mean() * 100\n",
    "        class_accuracies.append(class_acc)\n",
    "    \n",
    "    bars = plt.bar(class_names, class_accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Class-wise Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, class_accuracies):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{acc:.1f}%', ha='center', va='bottom')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Prediction Confidence Distribution\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    max_probs = np.max(all_probs, axis=1)\n",
    "    correct_mask = all_preds == all_targets\n",
    "    \n",
    "    plt.hist(max_probs[correct_mask], bins=30, alpha=0.7, label='Correct', color='green', density=True)\n",
    "    plt.hist(max_probs[~correct_mask], bins=30, alpha=0.7, label='Incorrect', color='red', density=True)\n",
    "    plt.xlabel('Prediction Confidence')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Confidence Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Enhanced Confusion Matrix with percentages\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both counts and percentages\n",
    "    annot = np.empty_like(cm, dtype=object)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)'\n",
    "    \n",
    "    sns.heatmap(cm_percent, annot=annot, fmt='', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Percentage'})\n",
    "    plt.title('Confusion Matrix (% and Counts)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    # 6. Model Calibration Plot\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    # Plot calibration curve for each class\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            all_targets_bin[:, i], all_probs[:, i], n_bins=10)\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", color=color,\n",
    "                 label=f'{class_names[i]}', linewidth=2, markersize=4)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend(loc=\"lower right\", fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PERFORMANCE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_acc = (all_preds == all_targets).mean() * 100\n",
    "    print(f\"Overall Test Accuracy: {overall_acc:.2f}%\")\n",
    "    print(f\"Average Confidence: {np.mean(max_probs):.3f}\")\n",
    "    print(f\"Confidence Std: {np.std(max_probs):.3f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Accuracy: {class_accuracies[i]:.2f}%\")\n",
    "        print(f\"  ROC AUC: {roc_auc[i]:.3f}\")\n",
    "        print(f\"  PR AUC: {pr_auc[i]:.3f}\")\n",
    "        \n",
    "        # Class-specific confidence stats\n",
    "        class_mask = all_targets == i\n",
    "        class_conf = max_probs[class_mask]\n",
    "        print(f\"  Avg Confidence: {np.mean(class_conf):.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'predictions': all_preds,\n",
    "        'probabilities': all_probs,\n",
    "        'targets': all_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a95af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the main function to include detailed analysis\n",
    "# Add this code at the end of the main() function, after the confusion matrix plotpython test_model_simple.py --model_path best_dabnn_model.pth --data_path /path/to/your/test/dataset\n",
    "\n",
    "def main_with_detailed_analysis():\n",
    "    \"\"\"Enhanced main function with detailed post-training analysis\"\"\"\n",
    "    # Set up data path (modify this to your dataset path)\n",
    "    data_path = \"/home/bruh/Documents/BNN2/split\"  # Change this to your actual dataset path\n",
    "    \n",
    "    # Verify CUDA optimization\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes\n",
    "        torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BNN Disease Classification Training with Detailed Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, class_names = create_data_loaders(data_path)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing DABNN model...\")\n",
    "    model = DABNN(num_classes=NUM_CLASSES).to(device)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for better generalization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Record history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1:3d}/{NUM_EPOCHS}] | \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"LR: {current_lr:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}%\"\n",
    "              f\"{' *' if val_acc == best_val_acc else ''}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epoch > 20 and val_acc < max(val_accs[-10:]) - 5:  # Stop if no improvement in 10 epochs\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nTesting best model...\")\n",
    "    test_acc, report, cm, preds, targets = test_model(model, test_loader, device, class_names)\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state,\n",
    "        'model_config': {\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'img_size': IMG_SIZE\n",
    "        },\n",
    "        'class_names': class_names,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc\n",
    "    }, 'best_dabnn_model.pth')\n",
    "    print(\"\\nBest model saved as 'best_dabnn_model.pth'\")\n",
    "    \n",
    "    # Plot basic results\n",
    "    print(\"\\nGenerating basic plots...\")\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "    \n",
    "    # DETAILED ANALYSIS - NEW ADDITION\n",
    "    print(\"\\nGenerating detailed performance analysis...\")\n",
    "    detailed_metrics = plot_detailed_analysis(model, test_loader, device, class_names)\n",
    "    \n",
    "    # Save detailed results\n",
    "    import json\n",
    "    detailed_results = {\n",
    "        'training_history': {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_accs': val_accs\n",
    "        },\n",
    "        'detailed_metrics': {\n",
    "            'roc_auc': {class_names[i]: float(detailed_metrics['roc_auc'][i]) for i in range(len(class_names))},\n",
    "            'pr_auc': {class_names[i]: float(detailed_metrics['pr_auc'][i]) for i in range(len(class_names))},\n",
    "            'class_accuracies': {class_names[i]: float(detailed_metrics['class_accuracies'][i]) for i in range(len(class_names))},\n",
    "            'overall_accuracy': float(detailed_metrics['overall_accuracy'])\n",
    "        },\n",
    "        'model_info': {\n",
    "            'total_params': sum(p.numel() for p in model.parameters()),\n",
    "            'model_size_mb': sum(p.numel() for p in model.parameters()) * 4 / (1024**2),\n",
    "            'binary_size_mb': sum(p.numel() for p in model.parameters()) / (8 * 1024**2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('detailed_training_results.json', 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2)\n",
    "    print(\"Detailed results saved to 'detailed_training_results.json'\")\n",
    "    \n",
    "    # Final model statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS WITH DETAILED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Total Training Time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Model Size (Binary): {sum(p.numel() for p in model.parameters()) / (8 * 1024**2):.2f} MB\")\n",
    "    \n",
    "    # Print summary of detailed metrics\n",
    "    print(f\"\\nDetailed Analysis Summary:\")\n",
    "    print(f\"Average ROC AUC: {np.mean(list(detailed_metrics['roc_auc'].values())):.3f}\")\n",
    "    print(f\"Average PR AUC: {np.mean(list(detailed_metrics['pr_auc'].values())):.3f}\")\n",
    "    print(f\"Class Accuracy Range: {min(detailed_metrics['class_accuracies']):.1f}% - {max(detailed_metrics['class_accuracies']):.1f}%\")\n",
    "    \n",
    "    return detailed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55adeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the enhanced analysis\n",
    "# Uncomment and run one of these options:\n",
    "\n",
    "# Option 1: Run the enhanced main function with detailed analysis\n",
    "# detailed_metrics = main_with_detailed_analysis()\n",
    "\n",
    "# Option 2: If you already have a trained model, load it and run detailed analysis only\n",
    "\"\"\"\n",
    "# Load pre-trained model\n",
    "checkpoint = torch.load('best_dabnn_model.pth', map_location=device)\n",
    "model = DABNN(num_classes=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "class_names = checkpoint['class_names']\n",
    "\n",
    "# Create test loader (you'll need to modify data_path)\n",
    "data_path = \"/home/bruh/Documents/BNN2/split\"  # Change this to your actual dataset path\n",
    "_, _, test_loader, _ = create_data_loaders(data_path)\n",
    "\n",
    "# Run detailed analysis on pre-trained model\n",
    "detailed_metrics = plot_detailed_analysis(model, test_loader, device, class_names)\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: Run original main function (without detailed analysis)\n",
    "# main()\n",
    "\n",
    "print(\"Choose one of the options above to run the analysis!\")\n",
    "print(\"- Option 1: Complete training with detailed analysis\")\n",
    "print(\"- Option 2: Load existing model and run detailed analysis only\") \n",
    "print(\"- Option 3: Run original training without detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
