{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08787279",
   "metadata": {},
   "source": [
    "# Binary Neural Network (BNN) for Plant Disease Classification\n",
    "\n",
    "This notebook implements a Binary Neural Network using PyTorch for multiclass plant disease classification. The BNN uses binary weights and activations to reduce model size and computational requirements while maintaining reasonable accuracy.\n",
    "\n",
    "## Features:\n",
    "- Binary weights and activations using sign function\n",
    "- Processes 128x128 RGB images (3×128×128 input) with memory-efficient loading\n",
    "- One hidden layer with binary weights\n",
    "- Multiclass output with softmax activation\n",
    "- CrossEntropyLoss for training\n",
    "- Memory-efficient data loading to prevent system hangs\n",
    "\n",
    "## Memory Optimizations:\n",
    "- Images loaded on-demand during training (not all at once)\n",
    "- Reduced batch size for 128x128 images\n",
    "- Multi-worker data loading for better performance\n",
    "- Efficient train/test splitting without loading full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "%pip install torch torchvision matplotlib numpy pandas seaborn scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for enhanced visualization and data export\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(\"Results directory created: ./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f334b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Activation Function\n",
    "class BinaryActivation(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Binary activation function using the sign function.\n",
    "    Forward: sign(x) = {-1 if x < 0, +1 if x >= 0}\n",
    "    Backward: Straight-through estimator (STE) - passes gradients through unchanged\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Apply sign function: -1 for negative, +1 for non-negative\n",
    "        return torch.sign(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through estimator: pass gradients through unchanged\n",
    "        # This allows gradients to flow back during training\n",
    "        return grad_output\n",
    "\n",
    "def binary_activation(x):\n",
    "    \"\"\"Wrapper function for binary activation\"\"\"\n",
    "    return BinaryActivation.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a570d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Linear Layer\n",
    "class BinaryLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Linear layer with binary weights.\n",
    "    Weights are binarized using the sign function during forward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights using normal distribution\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Binarize weights using sign function\n",
    "        binary_weight = torch.sign(self.weight)\n",
    "        \n",
    "        # Perform linear transformation with binary weights\n",
    "        output = F.linear(input, binary_weight, self.bias)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Neural Network Model\n",
    "class BinaryNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Neural Network for multiclass plant disease classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: Flattened 128x128 RGB images (3*128*128 = 49152 features)\n",
    "    - Hidden Layer: Binary linear layer with binary activation\n",
    "    - Output Layer: Regular linear layer for class logits\n",
    "    - Final: Softmax for multiclass prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=3*128*128, hidden_size=512, num_classes=4):\n",
    "        super(BinaryNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # First layer: Regular linear layer (input preprocessing)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Hidden layer: Binary linear layer\n",
    "        self.hidden_layer = BinaryLinear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer: Regular linear layer for final classification\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten input if it's not already flattened\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(x.size(0), -1)  # Flatten to (batch_size, input_size)\n",
    "        \n",
    "        # Input layer with ReLU activation\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden layer with binary weights and binary activation\n",
    "        x = self.hidden_layer(x)\n",
    "        x = binary_activation(x)  # Binary activation function\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation - raw logits)\n",
    "        logits = self.output_layer(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Get class probabilities using softmax\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Get predicted class labels\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training and Evaluation Functions with Early Stopping\n",
    "def train_bnn(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, \n",
    "             patience=20, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    Enhanced training function with detailed metrics collection and early stopping\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The neural network model to train\n",
    "    - train_loader: DataLoader for training data\n",
    "    - val_loader: DataLoader for validation data\n",
    "    - criterion: Loss function\n",
    "    - optimizer: Optimizer for updating weights\n",
    "    - device: Device to run training on (CPU/GPU)\n",
    "    - num_epochs: Maximum number of epochs to train\n",
    "    - patience: Number of epochs to wait for improvement before stopping\n",
    "    - min_delta: Minimum change in validation loss to qualify as improvement\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize tracking lists\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    epoch_times = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    # Detailed metrics per epoch\n",
    "    training_history = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    early_stopped = False\n",
    "    \n",
    "    print(f\"Starting training for up to {num_epochs} epochs with early stopping (patience={patience})...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        batch_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            batch_loss = loss.item()\n",
    "            running_loss += batch_loss\n",
    "            batch_losses.append(batch_loss)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_predictions / total_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        # Record time and learning rate\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        epoch_times.append(epoch_time)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Store detailed history\n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'time': epoch_time,\n",
    "            'learning_rate': current_lr,\n",
    "            'min_batch_loss': min(batch_losses),\n",
    "            'max_batch_loss': max(batch_losses),\n",
    "            'std_batch_loss': np.std(batch_losses)\n",
    "        })\n",
    "        \n",
    "        # Progress display\n",
    "        print(f'Epoch [{epoch+1:2d}/{num_epochs}] | Train Loss: {train_loss:.4f} | '\n",
    "              f'Train Acc: {train_accuracy:6.2f}% | Val Loss: {val_loss:.4f} | '\n",
    "              f'Val Acc: {val_accuracy:6.2f}% | Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"  ✓ Validation loss improved to {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  ✗ No improvement in validation loss for {patience_counter} epochs\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                early_stopped = True\n",
    "                break\n",
    "    \n",
    "    # Restore best model if early stopped\n",
    "    if early_stopped and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Restored model from best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
    "    if early_stopped:\n",
    "        print(f\"Early stopped at epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Average epoch time: {np.mean(epoch_times):.2f}s\")\n",
    "    \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, training_history, epoch_times, early_stopped\n",
    "\n",
    "def evaluate_bnn(model, test_loader, criterion, device, class_names):\n",
    "    \"\"\"\n",
    "    Enhanced evaluation function with detailed metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Store for detailed analysis\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct_predictions / total_samples\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    report = classification_report(all_targets, all_predictions, \n",
    "                                 target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        all_targets, all_predictions, average=None, labels=range(len(class_names))\n",
    "    )\n",
    "    \n",
    "    # Create detailed metrics dictionary\n",
    "    detailed_metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probabilities,\n",
    "        'classification_report': report,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_fscore': fscore,\n",
    "        'per_class_support': support\n",
    "    }\n",
    "    \n",
    "    return detailed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be05f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Real Plant Disease Dataset - Memory Efficient Version\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def load_plant_disease_dataset_efficient(dataset_path, image_size=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load the real plant disease dataset efficiently without loading all images into memory.\n",
    "    Dataset structure:\n",
    "    - dataset_path/healthy/\n",
    "    - dataset_path/rust/\n",
    "    - dataset_path/Soyabean_Mosaic/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define transforms for preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize to 128x128\n",
    "        transforms.ToTensor(),  # Convert to tensor and normalize to [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    # Load dataset using ImageFolder (this doesn't load images into memory yet)\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "    \n",
    "    print(\"Class mapping:\")\n",
    "    for idx, class_name in enumerate(dataset.classes):\n",
    "        print(f\"  {idx}: {class_name}\")\n",
    "    \n",
    "    # Count samples per class without loading images\n",
    "    class_counts = Counter()\n",
    "    for _, class_idx in dataset.samples:\n",
    "        class_counts[class_idx] += 1\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    for i, class_name in enumerate(dataset.classes):\n",
    "        count = class_counts[i]\n",
    "        print(f\"  {class_name}: {count} images\")\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    print(f\"\\nTotal samples: {total_samples}\")\n",
    "    \n",
    "    # Create data loader instead of loading all data into memory\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    return data_loader, dataset.classes, total_samples\n",
    "\n",
    "# Load your real plant disease dataset efficiently\n",
    "dataset_path = \"/home/bruh/Documents/drone-crop/ML/BNN-split-images/data\"\n",
    "print(\"Loading real plant disease dataset efficiently...\")\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"ERROR: Dataset path does not exist: {dataset_path}\")\n",
    "    print(\"Please check the path and try again.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load dataset efficiently\n",
    "        full_data_loader, class_names, total_samples = load_plant_disease_dataset_efficient(\n",
    "            dataset_path, image_size=128, batch_size=32\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset loaded successfully!\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Number of classes: {len(class_names)}\")\n",
    "        print(f\"Class names: {class_names}\")\n",
    "        print(f\"Data will be processed in batches of 32 images\")\n",
    "        \n",
    "        # Sample a small batch to get tensor shapes\n",
    "        sample_batch = next(iter(full_data_loader))\n",
    "        X_sample, y_sample = sample_batch\n",
    "        print(f\"\\nSample batch shape: {X_sample.shape}\")\n",
    "        print(f\"Sample labels shape: {y_sample.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Please check your dataset path and structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Test Split with Memory Efficient Approach\n",
    "%pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# Create train/val/test split using dataset indices\n",
    "def create_train_val_test_split(dataset, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/validation/test split without loading all data into memory\n",
    "    Train: 80%, Validation: 10%, Test: 10%\n",
    "    \"\"\"\n",
    "    # Get all labels for stratified split\n",
    "    all_labels = [dataset.samples[i][1] for i in range(len(dataset))]\n",
    "    \n",
    "    # Create indices for first split (train+val vs test)\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        indices, test_size=test_size, random_state=random_state, \n",
    "        stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    # Get labels for the train+val split\n",
    "    train_val_labels = [all_labels[i] for i in train_val_indices]\n",
    "    \n",
    "    # Further split train+val into train and validation\n",
    "    # Adjust validation size to account for the reduced dataset after removing test set\n",
    "    # val_size_adjusted = val_size / (1 - test_size)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices, test_size=val_size/(1-test_size), random_state=random_state, \n",
    "        stratify=train_val_labels\n",
    "    )\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_dataset = data_utils.Subset(dataset, train_indices)\n",
    "    val_dataset = data_utils.Subset(dataset, val_indices)\n",
    "    test_dataset = data_utils.Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Recreate the original dataset object for splitting\n",
    "dataset_path = \"/home/bruh/Documents/drone-crop/ML/BNN-split-images/data\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create train/val/test split (80%/10%/10%)\n",
    "train_dataset, val_dataset, test_dataset = create_train_val_test_split(\n",
    "    full_dataset, val_size=0.1, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} samples (80%)\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples (10%)\")\n",
    "print(f\"Test set: {len(test_dataset)} samples (10%)\")\n",
    "\n",
    "# Create data loaders with appropriate batch size for 128x128 images\n",
    "batch_size = 16  # Reduced batch size for 128x128 images\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Calculate class distribution in train/val/test sets\n",
    "# Make sure labels are stored as Python lists for counting\n",
    "train_labels = [full_dataset.samples[i][1] for i in train_dataset.indices]\n",
    "val_labels = [full_dataset.samples[i][1] for i in val_dataset.indices]\n",
    "test_labels = [full_dataset.samples[i][1] for i in test_dataset.indices]\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    count = train_labels.count(i)\n",
    "    print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "print(f\"\\nValidation set class distribution:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    count = val_labels.count(i)\n",
    "    print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    count = test_labels.count(i)\n",
    "    print(f\"  {class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Images from the Dataset\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Get sample images from the dataset\n",
    "def get_sample_images(dataset, class_names, samples_per_class=3):\n",
    "    \"\"\"Get sample images from each class\"\"\"\n",
    "    class_samples = {i: [] for i in range(len(class_names))}\n",
    "    \n",
    "    # Collect samples from each class\n",
    "    for idx, (img_path, class_idx) in enumerate(dataset.samples):\n",
    "        if len(class_samples[class_idx]) < samples_per_class:\n",
    "            class_samples[class_idx].append(idx)\n",
    "        \n",
    "        # Check if we have enough samples for all classes\n",
    "        if all(len(samples) >= samples_per_class for samples in class_samples.values()):\n",
    "            break\n",
    "    \n",
    "    return class_samples\n",
    "\n",
    "# Get sample indices\n",
    "sample_indices = get_sample_images(full_dataset, class_names, samples_per_class=3)\n",
    "\n",
    "# Display sample images from each class\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    indices = sample_indices[class_idx]\n",
    "    \n",
    "    for i, sample_idx in enumerate(indices):\n",
    "        plt.subplot(len(class_names), 3, class_idx * 3 + i + 1)\n",
    "        \n",
    "        # Load and process single image\n",
    "        img, label = full_dataset[sample_idx]\n",
    "        \n",
    "        # Denormalize image\n",
    "        img_denorm = denormalize_image(img)\n",
    "        img_denorm = torch.clamp(img_denorm, 0, 1)  # Ensure values are in [0,1]\n",
    "        \n",
    "        # Convert to numpy and transpose for matplotlib\n",
    "        img_np = img_denorm.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"{class_name}\\n(Class {class_idx})\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Images from Plant Disease Dataset', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample images from your plant disease dataset:\")\n",
    "print(f\"Classes: {', '.join(class_names)}\")\n",
    "print(f\"Image size: 128x128 pixels\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)} (80%)\")\n",
    "print(f\"Validation images: {len(val_dataset)} (10%)\")\n",
    "print(f\"Test images: {len(test_dataset)} (10%)\")\n",
    "\n",
    "# Memory usage info\n",
    "print(f\"\\nMemory-efficient loading:\")\n",
    "print(f\"- Images are loaded on-demand during training\")\n",
    "print(f\"- Batch size reduced to {batch_size} for 128x128 images\")\n",
    "print(f\"- Using {train_loader.num_workers} worker processes for data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Train the BNN Model\n",
    "# Model parameters\n",
    "input_size = 3 * 128 * 128  # 128x128 RGB images\n",
    "hidden_size = 512\n",
    "num_classes = len(class_names)  # Dynamic based on actual dataset\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Classes: {class_names}\")\n",
    "\n",
    "# Initialize model\n",
    "model = BinaryNeuralNetwork(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nBinary Neural Network Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Train the model with enhanced tracking and early stopping\n",
    "print(\"\\nStarting enhanced training with detailed metrics and early stopping...\")\n",
    "train_losses, train_accuracies, val_losses, val_accuracies, training_history, epoch_times, early_stopped = train_bnn(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=5,\n",
    "    patience=20,  # Stop if no improvement for 5 epochs\n",
    "    min_delta=0.001  # Minimum improvement threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Evaluation and Comprehensive Visualization\n",
    "print(\"\\nEvaluating model with detailed metrics...\")\n",
    "detailed_metrics = evaluate_bnn(model, test_loader, criterion, device, class_names)\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "plt.subplot(3, 4, 1)\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'g-', linewidth=2, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 4, 2)\n",
    "plt.plot(epochs, train_accuracies, 'r-', linewidth=2, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'g-', linewidth=2, label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 2. Training Time Analysis\n",
    "plt.subplot(3, 4, 3)\n",
    "plt.bar(range(1, len(epoch_times) + 1), epoch_times, alpha=0.7, color='green')\n",
    "plt.title('Training Time per Epoch', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "plt.subplot(3, 4, 4)\n",
    "cm = confusion_matrix(detailed_metrics['targets'], detailed_metrics['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# 4. Per-Class Performance\n",
    "plt.subplot(3, 4, 5)\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': detailed_metrics['per_class_precision'],\n",
    "    'Recall': detailed_metrics['per_class_recall'],\n",
    "    'F1-Score': detailed_metrics['per_class_fscore']\n",
    "}, index=class_names)\n",
    "\n",
    "metrics_df.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "plt.title('Per-Class Performance Metrics', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 5. Class Distribution\n",
    "plt.subplot(3, 4, 6)\n",
    "class_counts = [sum(1 for label in detailed_metrics['targets'] if label == i) for i in range(len(class_names))]\n",
    "plt.pie(class_counts, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Test Set Class Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 6. Loss Smoothness (Moving Average)\n",
    "plt.subplot(3, 4, 7)\n",
    "window_size = 5\n",
    "if len(train_losses) >= window_size:\n",
    "    smoothed_loss = pd.Series(train_losses).rolling(window=window_size).mean()\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', alpha=0.3, label='Raw Loss')\n",
    "    plt.plot(range(1, len(smoothed_loss) + 1), smoothed_loss, 'b-', linewidth=2, label=f'{window_size}-Epoch Moving Avg')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.title('Loss Smoothness Analysis', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Prediction Confidence Distribution\n",
    "plt.subplot(3, 4, 8)\n",
    "max_probs = [max(prob) for prob in detailed_metrics['probabilities']]\n",
    "plt.hist(max_probs, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "plt.title('Prediction Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Training vs Validation Accuracy\n",
    "plt.subplot(3, 4, 9)\n",
    "epochs = range(1, len(train_accuracies) + 1)\n",
    "plt.plot(epochs, train_accuracies, 'b-', linewidth=2, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'g-', linewidth=2, label='Validation Accuracy')\n",
    "plt.axvline(x=val_accuracies.index(max(val_accuracies))+1, color='red', linestyle='--', alpha=0.7)\n",
    "plt.title('Train vs Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 9. Training Metrics Summary\n",
    "plt.subplot(3, 4, 10)\n",
    "summary_metrics = [\n",
    "    detailed_metrics['test_accuracy'],\n",
    "    np.mean(detailed_metrics['per_class_precision']) * 100,\n",
    "    np.mean(detailed_metrics['per_class_recall']) * 100,\n",
    "    np.mean(detailed_metrics['per_class_fscore']) * 100\n",
    "]\n",
    "metric_names = ['Accuracy', 'Avg Precision', 'Avg Recall', 'Avg F1-Score']\n",
    "bars = plt.bar(metric_names, summary_metrics, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "plt.title('Model Performance Summary', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.ylim(0, 100)\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, summary_metrics):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 10. Error Analysis\n",
    "plt.subplot(3, 4, 11)\n",
    "correct_mask = np.array(detailed_metrics['targets']) == np.array(detailed_metrics['predictions'])\n",
    "correct_confidences = [max(prob) for i, prob in enumerate(detailed_metrics['probabilities']) if correct_mask[i]]\n",
    "incorrect_confidences = [max(prob) for i, prob in enumerate(detailed_metrics['probabilities']) if not correct_mask[i]]\n",
    "\n",
    "plt.hist(correct_confidences, bins=15, alpha=0.7, label='Correct Predictions', color='green')\n",
    "plt.hist(incorrect_confidences, bins=15, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "plt.title('Confidence: Correct vs Incorrect', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. Validation Loss Improvement and Early Stopping Analysis\n",
    "plt.subplot(3, 4, 12)\n",
    "val_loss_improvements = [-1 * (val_losses[i] - val_losses[i-1]) for i in range(1, len(val_losses))]\n",
    "plt.plot(range(2, len(val_losses) + 1), val_loss_improvements, 'purple', marker='o', linewidth=2)\n",
    "plt.title('Validation Loss Improvements', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Improvement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add early stopping marker if applicable\n",
    "if early_stopped:\n",
    "    stopped_epoch = len(val_losses)\n",
    "    plt.axvline(x=stopped_epoch, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.text(stopped_epoch - 0.5, max(val_loss_improvements)/2, 'Early Stopping', \n",
    "             rotation=90, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comprehensive_training_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Overall Test Accuracy: {detailed_metrics['test_accuracy']:.2f}%\")\n",
    "print(f\"Overall Test Loss: {detailed_metrics['test_loss']:.4f}\")\n",
    "print(f\"Total Training Time: {sum(epoch_times):.2f} seconds\")\n",
    "print(f\"Average Epoch Time: {np.mean(epoch_times):.2f} seconds\")\n",
    "if early_stopped:\n",
    "    print(f\"Early stopping activated after {len(train_losses)} epochs (patience=5)\")\n",
    "    print(f\"Best validation loss: {min(val_losses):.4f} at epoch {val_losses.index(min(val_losses))+1}\")\n",
    "print(f\"\\nPer-Class Performance:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Precision: {detailed_metrics['per_class_precision'][i]:.3f}\")\n",
    "    print(f\"    Recall: {detailed_metrics['per_class_recall'][i]:.3f}\")\n",
    "    print(f\"    F1-Score: {detailed_metrics['per_class_fscore'][i]:.3f}\")\n",
    "    print(f\"    Support: {detailed_metrics['per_class_support'][i]}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Test model predictions on a few samples\n",
    "print(\"\\nTesting model predictions:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a small batch from test dataset\n",
    "    test_iter = iter(test_loader)\n",
    "    test_batch = next(test_iter)\n",
    "    test_samples, test_labels = test_batch\n",
    "    \n",
    "    # Select just the first 5 samples\n",
    "    test_samples = test_samples[:5].to(device)\n",
    "    test_labels = test_labels[:5].to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    logits = model(test_samples)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_classes = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i in range(min(5, len(test_samples))):\n",
    "        true_class_name = class_names[test_labels[i].item()]\n",
    "        pred_class_name = class_names[predicted_classes[i].item()]\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  True label: {test_labels[i].item()} ({true_class_name})\")\n",
    "        print(f\"  Predicted: {predicted_classes[i].item()} ({pred_class_name})\")\n",
    "        print(f\"  Probabilities: {probabilities[i].cpu().numpy()}\")\n",
    "        \n",
    "        # Show probability for each class\n",
    "        for j, class_name in enumerate(class_names):\n",
    "            prob = probabilities[i][j].item()\n",
    "            print(f\"    {class_name}: {prob:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive CSV Data Export\n",
    "print(\"\\nExporting training data to CSV files...\")\n",
    "\n",
    "# 1. Training History CSV\n",
    "training_df = pd.DataFrame(training_history)\n",
    "training_df.to_csv('results/training_history.csv', index=False)\n",
    "print(\"✓ Training history saved to: results/training_history.csv\")\n",
    "\n",
    "# 2. Detailed Test Results CSV\n",
    "test_results = []\n",
    "for i in range(len(detailed_metrics['targets'])):\n",
    "    test_results.append({\n",
    "        'sample_id': i,\n",
    "        'true_label': detailed_metrics['targets'][i],\n",
    "        'true_class': class_names[detailed_metrics['targets'][i]],\n",
    "        'predicted_label': detailed_metrics['predictions'][i],\n",
    "        'predicted_class': class_names[detailed_metrics['predictions'][i]],\n",
    "        'correct': detailed_metrics['targets'][i] == detailed_metrics['predictions'][i],\n",
    "        'confidence': max(detailed_metrics['probabilities'][i]),\n",
    "        **{f'prob_{class_names[j]}': detailed_metrics['probabilities'][i][j] for j in range(len(class_names))}\n",
    "    })\n",
    "\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv('results/test_predictions.csv', index=False)\n",
    "print(\"✓ Test predictions saved to: results/test_predictions.csv\")\n",
    "\n",
    "# 3. Per-Class Performance Metrics CSV\n",
    "per_class_metrics = pd.DataFrame({\n",
    "    'class_name': class_names,\n",
    "    'precision': detailed_metrics['per_class_precision'],\n",
    "    'recall': detailed_metrics['per_class_recall'],\n",
    "    'f1_score': detailed_metrics['per_class_fscore'],\n",
    "    'support': detailed_metrics['per_class_support']\n",
    "})\n",
    "per_class_metrics.to_csv('results/per_class_metrics.csv', index=False)\n",
    "print(\"✓ Per-class metrics saved to: results/per_class_metrics.csv\")\n",
    "\n",
    "# 4. Model Configuration and Final Results CSV\n",
    "model_summary = {\n",
    "    'parameter': [\n",
    "        'model_type', 'input_size', 'hidden_size', 'num_classes',\n",
    "        'num_epochs', 'batch_size', 'learning_rate', 'optimizer',\n",
    "        'total_parameters', 'trainable_parameters',\n",
    "        'final_train_loss', 'final_train_accuracy', 'test_loss', 'test_accuracy',\n",
    "        'total_training_time', 'avg_epoch_time',\n",
    "        'dataset_total_samples', 'train_samples', 'val_samples', 'test_samples',\n",
    "    'early_stopping', 'early_stopping_epoch'\n",
    "    ],\n",
    "    'value': [\n",
    "        'Binary Neural Network', input_size, hidden_size, num_classes,\n",
    "        40, batch_size, 0.0001, 'Adam',\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        train_losses[-1], train_accuracies[-1], \n",
    "        detailed_metrics['test_loss'], detailed_metrics['test_accuracy'],\n",
    "        sum(epoch_times), np.mean(epoch_times),\n",
    "        len(full_dataset), len(train_dataset), len(val_dataset), len(test_dataset),\n",
    "        'Yes' if early_stopped else 'No', len(train_losses) if early_stopped else 40\n",
    "    ]\n",
    "}\n",
    "model_summary_df = pd.DataFrame(model_summary)\n",
    "model_summary_df.to_csv('results/model_summary.csv', index=False)\n",
    "print(\"✓ Model summary saved to: results/model_summary.csv\")\n",
    "\n",
    "# 5. Confusion Matrix CSV\n",
    "cm = confusion_matrix(detailed_metrics['targets'], detailed_metrics['predictions'])\n",
    "cm_df = pd.DataFrame(cm, index=[f'True_{name}' for name in class_names], \n",
    "                     columns=[f'Pred_{name}' for name in class_names])\n",
    "cm_df.to_csv('results/confusion_matrix.csv')\n",
    "print(\"✓ Confusion matrix saved to: results/confusion_matrix.csv\")\n",
    "\n",
    "# 6. Training Progress Summary CSV with Validation Data\n",
    "epoch_summary = []\n",
    "for i in range(len(train_losses)):\n",
    "    epoch_summary.append({\n",
    "        'epoch': i + 1,\n",
    "        'train_loss': train_losses[i],\n",
    "        'train_accuracy': train_accuracies[i],\n",
    "        'val_loss': val_losses[i],\n",
    "        'val_accuracy': val_accuracies[i],\n",
    "        'epoch_time': epoch_times[i],\n",
    "        'cumulative_time': sum(epoch_times[:i+1]),\n",
    "        'train_loss_improvement': 0 if i == 0 else train_losses[i-1] - train_losses[i],\n",
    "        'val_loss_improvement': 0 if i == 0 else val_losses[i-1] - val_losses[i],\n",
    "        'is_best_model': val_losses[i] == min(val_losses[:i+1])\n",
    "    })\n",
    "\n",
    "epoch_summary_df = pd.DataFrame(epoch_summary)\n",
    "epoch_summary_df.to_csv('results/epoch_summary.csv', index=False)\n",
    "print(\"✓ Epoch summary saved to: results/epoch_summary.csv\")\n",
    "\n",
    "# 7. Binary Weights Analysis CSV with Early Stopping Info\n",
    "with torch.no_grad():\n",
    "    hidden_weights = model.hidden_layer.weight.data.cpu().numpy()\n",
    "    binary_weights = np.sign(hidden_weights)\n",
    "    \n",
    "    weights_analysis = {\n",
    "        'layer': ['hidden_layer'],\n",
    "        'total_weights': [hidden_weights.size],\n",
    "        'positive_weights': [np.sum(binary_weights > 0)],\n",
    "        'negative_weights': [np.sum(binary_weights < 0)],\n",
    "        'zero_weights': [np.sum(binary_weights == 0)],\n",
    "        'weight_mean': [np.mean(hidden_weights)],\n",
    "        'weight_std': [np.std(hidden_weights)],\n",
    "        'binary_weight_ratio': [np.sum(binary_weights > 0) / hidden_weights.size],\n",
    "        'early_stopping_active': ['Yes' if early_stopped else 'No'],\n",
    "        'completed_epochs': [len(train_losses)],\n",
    "        'best_val_loss_epoch': [val_losses.index(min(val_losses)) + 1]\n",
    "    }\n",
    "    \n",
    "weights_analysis_df = pd.DataFrame(weights_analysis)\n",
    "weights_analysis_df.to_csv('results/binary_weights_analysis.csv', index=False)\n",
    "print(\"✓ Binary weights analysis saved to: results/binary_weights_analysis.csv\")\n",
    "\n",
    "# 8. Dataset Statistics CSV with Train/Val/Test Split\n",
    "# Convert labels to numpy arrays for consistent handling\n",
    "def ensure_numpy_array(labels):\n",
    "    \"\"\"Convert labels to numpy arrays regardless of input type (list, tensor, etc.)\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        return labels.cpu().numpy()\n",
    "    elif isinstance(labels, np.ndarray):\n",
    "        return labels\n",
    "    else:\n",
    "        return np.array(labels)\n",
    "\n",
    "# Convert all label sets to numpy arrays\n",
    "train_labels_np = ensure_numpy_array(train_labels)\n",
    "val_labels_np = ensure_numpy_array(val_labels)\n",
    "test_labels_np = ensure_numpy_array(test_labels)\n",
    "\n",
    "dataset_stats = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Count occurrences using numpy\n",
    "    train_count = np.sum(train_labels_np == i)\n",
    "    val_count = np.sum(val_labels_np == i)\n",
    "    test_count = np.sum(test_labels_np == i)\n",
    "    total_count = train_count + val_count + test_count\n",
    "    \n",
    "    dataset_stats.append({\n",
    "        'class_name': class_name,\n",
    "        'class_id': i,\n",
    "        'train_samples': int(train_count),  # Convert to int to avoid numpy type issues\n",
    "        'val_samples': int(val_count),\n",
    "        'test_samples': int(test_count),\n",
    "        'total_samples': int(total_count),\n",
    "        'train_percentage': (train_count / len(train_labels_np)) * 100,\n",
    "        'val_percentage': (val_count / len(val_labels_np)) * 100,\n",
    "        'test_percentage': (test_count / len(test_labels_np)) * 100,\n",
    "        'overall_percentage': (total_count / len(full_dataset)) * 100\n",
    "    })\n",
    "\n",
    "dataset_stats_df = pd.DataFrame(dataset_stats)\n",
    "dataset_stats_df.to_csv('results/dataset_statistics.csv', index=False)\n",
    "print(\"✓ Dataset statistics saved to: results/dataset_statistics.csv\")\n",
    "\n",
    "# Create a comprehensive summary report\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CSV FILES EXPORTED SUCCESSFULLY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"The following CSV files have been created in the 'results/' directory:\")\n",
    "print(\"1. training_history.csv - Detailed epoch-by-epoch training metrics\")\n",
    "print(\"2. test_predictions.csv - Individual test sample predictions and probabilities\")\n",
    "print(\"3. per_class_metrics.csv - Precision, recall, F1-score for each class\")\n",
    "print(\"4. model_summary.csv - Model configuration and final performance\")\n",
    "print(\"5. confusion_matrix.csv - Confusion matrix data\")\n",
    "print(\"6. epoch_summary.csv - Training and validation progress with improvements\")\n",
    "print(\"7. binary_weights_analysis.csv - Analysis of binary weight distribution with early stopping info\")\n",
    "print(\"8. dataset_statistics.csv - Dataset composition and train/val/test distribution\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Display sample of key CSV files\n",
    "print(\"\\nSample of Training History:\")\n",
    "print(training_df.head())\n",
    "print(\"\\nSample of Test Predictions:\")\n",
    "print(test_results_df.head())\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(per_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c781770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL BINARY NEURAL NETWORK PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: Binary Neural Network for Plant Disease Classification\")\n",
    "print(f\"Dataset: {len(class_names)} classes - {', '.join(class_names)}\")\n",
    "print(f\"Total Images: {len(full_dataset)} (Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)})\")\n",
    "print(f\"Image Size: 128x128 RGB\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Max Training Epochs: 40\")\n",
    "print(f\"Actual Epochs Run: {len(train_losses)}\")\n",
    "print(f\"Early Stopping: {'Activated' if early_stopped else 'Not triggered'}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f} (Epoch {val_losses.index(min(val_losses))+1})\")\n",
    "print(f\"Training Time: {sum(epoch_times):.2f} seconds\")\n",
    "print(f\"\")\n",
    "print(f\"FINAL RESULTS:\")\n",
    "print(f\"  Test Accuracy: {detailed_metrics['test_accuracy']:.2f}%\")\n",
    "print(f\"  Test Loss: {detailed_metrics['test_loss']:.4f}\")\n",
    "print(f\"  Average Precision: {np.mean(detailed_metrics['per_class_precision']):.3f}\")\n",
    "print(f\"  Average Recall: {np.mean(detailed_metrics['per_class_recall']):.3f}\")\n",
    "print(f\"  Average F1-Score: {np.mean(detailed_metrics['per_class_fscore']):.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"MODEL COMPRESSION:\")\n",
    "binary_params = sum(p.numel() for name, p in model.named_parameters() if 'hidden_layer.weight' in name)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Binary Parameters: {binary_params:,} ({binary_params/total_params:.1%} of total)\")\n",
    "print(f\"  Theoretical Storage Reduction: ~{binary_params * 32 / total_params:.1f}x for binary weights\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"All results, graphs, and CSV files have been saved to the 'results/' directory.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Analysis and Binary Weights Visualization\n",
    "def analyze_binary_weights(model):\n",
    "    \"\"\"Analyze the binary weights in the model\"\"\"\n",
    "    print(\"Binary Weights Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze hidden layer binary weights\n",
    "    with torch.no_grad():\n",
    "        hidden_weights = model.hidden_layer.weight\n",
    "        binary_weights = torch.sign(hidden_weights)\n",
    "        \n",
    "        print(f\"Hidden layer weights shape: {hidden_weights.shape}\")\n",
    "        print(f\"Original weights - Mean: {hidden_weights.mean():.4f}, Std: {hidden_weights.std():.4f}\")\n",
    "        print(f\"Binary weights - Unique values: {torch.unique(binary_weights)}\")\n",
    "        print(f\"Binary weights - Distribution: +1: {(binary_weights == 1).sum().item()}, -1: {(binary_weights == -1).sum().item()}\")\n",
    "        \n",
    "        # Plot weight distributions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(hidden_weights.cpu().numpy().flatten(), bins=50, alpha=0.7, color='blue')\n",
    "        plt.title('Original Weights Distribution')\n",
    "        plt.xlabel('Weight Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(binary_weights.cpu().numpy().flatten(), bins=3, alpha=0.7, color='red')\n",
    "        plt.title('Binary Weights Distribution')\n",
    "        plt.xlabel('Binary Weight Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze the trained model\n",
    "analyze_binary_weights(model)\n",
    "\n",
    "# Calculate model size reduction\n",
    "def calculate_model_compression():\n",
    "    \"\"\"Calculate the compression achieved by using binary weights\"\"\"\n",
    "    \n",
    "    # Count parameters in binary layers\n",
    "    binary_params = sum(p.numel() for name, p in model.named_parameters() \n",
    "                       if 'hidden_layer.weight' in name)\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nModel Compression Analysis:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Binary parameters: {binary_params:,}\")\n",
    "    print(f\"Binary ratio: {binary_params/total_params:.2%}\")\n",
    "    \n",
    "    # In practice, binary weights can be stored using 1 bit vs 32 bits (float32)\n",
    "    # This gives approximately 32x compression for binary weights\n",
    "    theoretical_compression = binary_params * 32 / (total_params * 32 - binary_params * 31)\n",
    "    print(f\"Theoretical storage compression: {theoretical_compression:.1f}x\")\n",
    "\n",
    "calculate_model_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9571db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete model saved to: model_weights/bnn_model_20250717_105644.pt\n",
      "✓ Model weights saved to: model_weights/bnn_weights_20250717_105644.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't redefine method: forward on class: __torch__.BinaryNeuralNetwork (of Python compilation unit at: 0x565402381a80)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Save in optimized TorchScript format for deployment\u001b[39;00m\n\u001b[32m     22\u001b[39m scripted_model_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel_weights/bnn_scripted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m scripted_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m scripted_model.save(scripted_model_path)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ TorchScript model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscripted_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/drone-crop/ML/venv1/lib/python3.13/site-packages/torch/jit/_script.py:1443\u001b[39m, in \u001b[36mscript\u001b[39m\u001b[34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[39m\n\u001b[32m   1441\u001b[39m prev = _TOPLEVEL\n\u001b[32m   1442\u001b[39m _TOPLEVEL = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m ret = \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[32m   1452\u001b[39m     log_torchscript_usage(\u001b[33m\"\u001b[39m\u001b[33mscript\u001b[39m\u001b[33m\"\u001b[39m, model_id=_get_model_id(ret))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/drone-crop/ML/venv1/lib/python3.13/site-packages/torch/jit/_script.py:1152\u001b[39m, in \u001b[36m_script_impl\u001b[39m\u001b[34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[39m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.nn.Module):\n\u001b[32m   1151\u001b[39m     obj = call_prepare_scriptable_func(obj)\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_recursive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_recursive\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1156\u001b[39m     obj = (\n\u001b[32m   1157\u001b[39m         obj.__prepare_scriptable__()\n\u001b[32m   1158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33m__prepare_scriptable__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[32m   1160\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/drone-crop/ML/venv1/lib/python3.13/site-packages/torch/jit/_recursive.py:556\u001b[39m, in \u001b[36mcreate_script_module\u001b[39m\u001b[34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[32m    555\u001b[39m     AttributeTypeIsSupportedChecker().check(nn_module)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/drone-crop/ML/venv1/lib/python3.13/site-packages/torch/jit/_recursive.py:629\u001b[39m, in \u001b[36mcreate_script_module_impl\u001b[39m\u001b[34m(nn_module, concrete_type, stubs_fn)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store.methods_compiled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[32m    633\u001b[39m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[32m    634\u001b[39m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/drone-crop/ML/venv1/lib/python3.13/site-packages/torch/jit/_recursive.py:465\u001b[39m, in \u001b[36mcreate_methods_and_properties_from_stubs\u001b[39m\u001b[34m(concrete_type, method_stubs, property_stubs)\u001b[39m\n\u001b[32m    462\u001b[39m property_defs = [p.def_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[32m    463\u001b[39m property_rcbs = [p.resolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[43mconcrete_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't redefine method: forward on class: __torch__.BinaryNeuralNetwork (of Python compilation unit at: 0x565402381a80)"
     ]
    }
   ],
   "source": [
    "# Save Model Weights\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create a directory for model weights if it doesn't exist\n",
    "os.makedirs('model_weights', exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique filename\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the complete model\n",
    "model_path = f\"model_weights/bnn_model_{timestamp}.pt\"\n",
    "torch.save(model, model_path)\n",
    "print(f\"✓ Complete model saved to: {model_path}\")\n",
    "\n",
    "# Save just the state dictionary (more portable)\n",
    "state_dict_path = f\"model_weights/bnn_weights_{timestamp}.pth\"\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(f\"✓ Model weights saved to: {state_dict_path}\")\n",
    "\n",
    "# Save in optimized TorchScript format for deployment\n",
    "scripted_model_path = f\"model_weights/bnn_scripted_{timestamp}.pt\"\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(scripted_model_path)\n",
    "print(f\"✓ TorchScript model saved to: {scripted_model_path}\")\n",
    "\n",
    "print(\"\\nModel Information Summary:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Output classes: {num_classes}\")\n",
    "print(f\"  Class names: {class_names}\")\n",
    "print(f\"  Test accuracy: {detailed_metrics['test_accuracy']:.2f}%\")\n",
    "\n",
    "# Save model architecture and metadata as JSON for reference\n",
    "import json\n",
    "model_info = {\n",
    "    'timestamp': timestamp,\n",
    "    'model_type': 'BinaryNeuralNetwork',\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names,\n",
    "    'test_accuracy': float(detailed_metrics['test_accuracy']),\n",
    "    'test_loss': float(detailed_metrics['test_loss']),\n",
    "    'training_epochs': len(train_losses),\n",
    "    'early_stopped': bool(early_stopped),\n",
    "    'best_epoch': int(val_losses.index(min(val_losses)) + 1)\n",
    "}\n",
    "\n",
    "with open(f\"model_weights/bnn_metadata_{timestamp}.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "print(f\"✓ Model metadata saved to: model_weights/bnn_metadata_{timestamp}.json\")\n",
    "\n",
    "print(\"\\nTo load this model in the future:\")\n",
    "print(\"```python\")\n",
    "print(\"# Option 1: Load complete model\")\n",
    "print(f\"loaded_model = torch.load('{model_path}')\")\n",
    "print(\"loaded_model.eval()\")\n",
    "print(\"\")\n",
    "print(\"# Option 2: Load state dictionary into a new model instance\")\n",
    "print(\"new_model = BinaryNeuralNetwork(input_size, hidden_size, num_classes).to(device)\")\n",
    "print(f\"new_model.load_state_dict(torch.load('{state_dict_path}'))\")\n",
    "print(\"new_model.eval()\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78538d6",
   "metadata": {},
   "source": [
    "## Model Loading and Inference Guide\n",
    "\n",
    "After saving the model, you can load it for inference using these approaches:\n",
    "\n",
    "### Option 1: Load Complete Model\n",
    "```python\n",
    "# Load the entire model (architecture + weights)\n",
    "loaded_model = torch.load('model_weights/bnn_model_YYYYMMDD_HHMMSS.pt')\n",
    "loaded_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input\n",
    "    output = loaded_model(input_tensor)\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    print(f\"Predicted class: {class_names[predicted_class]}\")\n",
    "```\n",
    "\n",
    "### Option 2: Load State Dictionary into New Model Instance\n",
    "```python\n",
    "# First create a model with the same architecture\n",
    "new_model = BinaryNeuralNetwork(\n",
    "    input_size=3*128*128, \n",
    "    hidden_size=512, \n",
    "    num_classes=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Then load the weights\n",
    "new_model.load_state_dict(torch.load('model_weights/bnn_weights_YYYYMMDD_HHMMSS.pth'))\n",
    "new_model.eval()\n",
    "```\n",
    "\n",
    "### Option 3: Use TorchScript Model for Deployment\n",
    "```python\n",
    "# Load scripted model (optimized for deployment)\n",
    "scripted_model = torch.jit.load('model_weights/bnn_scripted_YYYYMMDD_HHMMSS.pt')\n",
    "scripted_model.eval()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1, 3, 128, 128).to(device)\n",
    "    output = scripted_model(input_tensor)\n",
    "    # Process output...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reusable functions for model saving and loading\n",
    "def save_bnn_model(model, class_names, input_size, hidden_size, metrics=None, folder=\"model_weights\"):\n",
    "    \"\"\"\n",
    "    Save a Binary Neural Network model in multiple formats with metadata\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to save\n",
    "        class_names: List of class names\n",
    "        input_size: Input dimension size\n",
    "        hidden_size: Hidden layer size\n",
    "        metrics: Optional dictionary of evaluation metrics\n",
    "        folder: Folder to save the model\n",
    "    \n",
    "    Returns:\n",
    "        timestamp: The timestamp used in filenames\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import json\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save complete model\n",
    "    model_path = f\"{folder}/bnn_model_{timestamp}.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    print(f\"✓ Complete model saved to: {model_path}\")\n",
    "    \n",
    "    # Save state dictionary\n",
    "    state_dict_path = f\"{folder}/bnn_weights_{timestamp}.pth\"\n",
    "    torch.save(model.state_dict(), state_dict_path)\n",
    "    print(f\"✓ Model weights saved to: {state_dict_path}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_type': 'BinaryNeuralNetwork',\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_classes': len(class_names),\n",
    "        'class_names': class_names,\n",
    "    }\n",
    "    \n",
    "    # Add metrics if provided\n",
    "    if metrics:\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float, bool, str)):\n",
    "                metadata[key] = value\n",
    "            elif hasattr(value, 'item'):\n",
    "                metadata[key] = value.item()  # For tensor scalars\n",
    "    \n",
    "    metadata_path = f\"{folder}/bnn_metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"✓ Model metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "def load_bnn_model(model_path=None, weights_path=None, metadata_path=None):\n",
    "    \"\"\"\n",
    "    Load a Binary Neural Network model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to complete model file (.pt)\n",
    "        weights_path: Path to model weights file (.pth)\n",
    "        metadata_path: Path to model metadata file (.json)\n",
    "        \n",
    "    Returns:\n",
    "        model: The loaded model\n",
    "        metadata: Dictionary of model metadata if available\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    metadata = None\n",
    "    \n",
    "    # Load metadata if provided\n",
    "    if metadata_path:\n",
    "        try:\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            print(f\"✓ Loaded model metadata from {metadata_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load metadata: {e}\")\n",
    "    \n",
    "    # Load complete model if path provided\n",
    "    if model_path:\n",
    "        try:\n",
    "            model = torch.load(model_path)\n",
    "            print(f\"✓ Loaded complete model from {model_path}\")\n",
    "            return model, metadata\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading complete model: {e}\")\n",
    "    \n",
    "    # If weights path is provided but not model_path, or if model_path loading failed\n",
    "    if weights_path:\n",
    "        try:\n",
    "            if metadata:\n",
    "                # Create model based on metadata\n",
    "                model = BinaryNeuralNetwork(\n",
    "                    input_size=metadata['input_size'],\n",
    "                    hidden_size=metadata['hidden_size'],\n",
    "                    num_classes=len(metadata['class_names'])\n",
    "                ).to(device)\n",
    "                print(f\"✓ Created model structure from metadata\")\n",
    "            else:\n",
    "                print(\"Warning: No metadata provided. Using default model structure.\")\n",
    "                model = BinaryNeuralNetwork().to(device)\n",
    "                \n",
    "            # Load weights\n",
    "            model.load_state_dict(torch.load(weights_path))\n",
    "            print(f\"✓ Loaded model weights from {weights_path}\")\n",
    "            return model, metadata\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model weights: {e}\")\n",
    "    \n",
    "    # If we get here, loading failed\n",
    "    print(\"Failed to load model\")\n",
    "    return None, metadata\n",
    "\n",
    "# Example of using the function (commented out)\n",
    "\"\"\"\n",
    "# To save a model:\n",
    "timestamp = save_bnn_model(\n",
    "    model=model, \n",
    "    class_names=class_names,\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size,\n",
    "    metrics={\n",
    "        'test_accuracy': detailed_metrics['test_accuracy'],\n",
    "        'test_loss': detailed_metrics['test_loss'],\n",
    "        'early_stopped': early_stopped,\n",
    "        'epochs_trained': len(train_losses)\n",
    "    }\n",
    ")\n",
    "\n",
    "# To load a model:\n",
    "folder = \"model_weights\"\n",
    "timestamp = \"20250717_120000\"  # example timestamp\n",
    "loaded_model, metadata = load_bnn_model(\n",
    "    model_path=f\"{folder}/bnn_model_{timestamp}.pt\",\n",
    "    metadata_path=f\"{folder}/bnn_metadata_{timestamp}.json\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the currently trained model\n",
    "print(\"Saving the trained BNN model...\")\n",
    "\n",
    "# Save using the function\n",
    "timestamp = save_bnn_model(\n",
    "    model=model, \n",
    "    class_names=class_names,\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size,\n",
    "    metrics={\n",
    "        'test_accuracy': detailed_metrics['test_accuracy'],\n",
    "        'test_loss': detailed_metrics['test_loss'],\n",
    "        'best_val_loss': min(val_losses),\n",
    "        'best_val_accuracy': max(val_accuracies),\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_train_accuracy': train_accuracies[-1],\n",
    "        'early_stopped': early_stopped,\n",
    "        'epochs_trained': len(train_losses),\n",
    "        'best_epoch': val_losses.index(min(val_losses)) + 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nModel saved with timestamp: {timestamp}\")\n",
    "print(f\"You can use this timestamp to load the model later.\")\n",
    "print(f\"All model files are in the 'model_weights' directory.\")\n",
    "\n",
    "# Create a simple example inference script\n",
    "inference_script = f\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "# Load metadata\n",
    "with open('model_weights/bnn_metadata_{timestamp}.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Define the same model architecture\n",
    "class BinaryActivation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "def binary_activation(x):\n",
    "    return BinaryActivation.apply(x)\n",
    "\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        binary_weight = torch.sign(self.weight)\n",
    "        return F.linear(input, binary_weight, self.bias)\n",
    "\n",
    "class BinaryNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=3*128*128, hidden_size=512, num_classes=4):\n",
    "        super(BinaryNeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = BinaryLinear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = binary_activation(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize model with metadata parameters\n",
    "model = BinaryNeuralNetwork(\n",
    "    input_size=metadata['input_size'],\n",
    "    hidden_size=metadata['hidden_size'],\n",
    "    num_classes=len(metadata['class_names'])\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load('model_weights/bnn_weights_{timestamp}.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    # Get results\n",
    "    class_name = metadata['class_names'][predicted_class]\n",
    "    confidence = probabilities[predicted_class].item()\n",
    "    \n",
    "    print(f\"Prediction: {{class_name}} (Confidence: {{confidence:.2f}})\")\n",
    "    \n",
    "    # Return all class probabilities\n",
    "    for i, class_name in enumerate(metadata['class_names']):\n",
    "        prob = probabilities[i].item()\n",
    "        print(f\"  {{class_name}}: {{prob:.4f}}\")\n",
    "    \n",
    "    return predicted_class, confidence, probabilities.tolist()\n",
    "\n",
    "# Example usage\n",
    "# predict_image('path/to/your/image.jpg')\n",
    "\"\"\"\n",
    "\n",
    "# Save the inference script\n",
    "with open(f\"model_weights/bnn_inference_{timestamp}.py\", \"w\") as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(f\"\\n✓ Inference script created: model_weights/bnn_inference_{timestamp}.py\")\n",
    "print(\"You can use this script to make predictions with your saved model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fe942",
   "metadata": {},
   "source": [
    "## Using Real Plant Disease Data\n",
    "\n",
    "To use this BNN with real plant disease images, follow these steps:\n",
    "\n",
    "### 1. Data Preparation\n",
    "```python\n",
    "# Example for loading real plant disease dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define transforms for 128x128 RGB images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load your dataset (example structure)\n",
    "# dataset/\n",
    "#   ├── healthy/\n",
    "#   ├── disease1/\n",
    "#   └── disease2/\n",
    "\n",
    "dataset = datasets.ImageFolder(root='path/to/your/dataset', \n",
    "                              transform=transform)\n",
    "```\n",
    "\n",
    "### 2. Model Adaptation\n",
    "- Adjust `num_classes` parameter based on your dataset\n",
    "- Modify `hidden_size` for different model complexities\n",
    "- Consider adding more binary layers for deeper networks\n",
    "- Note: 128x128 images require 4x more parameters than 64x64 (49,152 vs 12,288 input features)\n",
    "\n",
    "### 3. Training Tips\n",
    "- Use data augmentation for better generalization\n",
    "- Implement learning rate scheduling\n",
    "- Add early stopping to prevent overfitting\n",
    "- Consider using batch normalization before binary activations\n",
    "- Higher resolution images may require longer training times\n",
    "\n",
    "### 4. Performance Optimization\n",
    "- Experiment with different optimizers (SGD, AdamW)\n",
    "- Try different initialization strategies for binary weights\n",
    "- Implement gradient clipping for stable training\n",
    "- Use mixed precision training for faster computation\n",
    "- Consider reducing batch size for 128x128 images if memory is limited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
